---
title: 通过示例了解 Azure 数据出厂定价
description: 本文使用详细的示例介绍并演示 Azure 数据工厂定价模型
documentationcenter: ''
author: djpmsft
ms.author: daperlov
manager: jroth
ms.reviewer: maghan
ms.service: data-factory
ms.workload: data-services
ms.topic: conceptual
ms.date: 12/27/2019
ms.openlocfilehash: ee5acc97e4b05a0e93f4ceee8c04b400da211b49
ms.sourcegitcommit: 2ec4b3d0bad7dc0071400c2a2264399e4fe34897
ms.translationtype: MT
ms.contentlocale: zh-CN
ms.lasthandoff: 03/27/2020
ms.locfileid: "76769495"
---
# <a name="understanding-data-factory-pricing-through-examples"></a>通过示例了解数据工厂定价

本文使用详细的示例介绍并演示 Azure 数据工厂定价模型。

> [!NOTE]
> 以下这些示例中使用的价格是假设的，并不意味着实际定价。

## <a name="copy-data-from-aws-s3-to-azure-blob-storage-hourly"></a>将数据每隔一小时从 AWS S3 复制到 Azure Blob 存储

在此方案中，需按计划将数据每隔一小时从 AWS S3 复制到 Azure Blob 存储。

若要完成此方案，需使用以下项创建一个管道：

1. 使用输入数据集（适用于将要从 AWS S3 复制的数据）的复制活动。

2. Azure 存储上的数据的输出数据集。

3. 一个计划触发器，用于每隔一小时执行一次管道。

   ![方案 1](media/pricing-concepts/scenario1.png)

| **操作** | **类型和单元** |
| --- | --- |
| 创建链接的服务 | 2 个读/写实体  |
| 创建数据集 | 4 个读/写实体（2 个用于创建数据集，2 个用于链接的服务的引用） |
| 创建管道 | 3 个读/写实体（1 个用于创建管道，2 个用于数据集引用） |
| 获取管道 | 1 个读/写实体 |
| 运行管道 | 2 个活动运行（1 个用于触发器运行，1 个用于活动运行） |
| 复制数据假设：执行时间 = 10 分钟 | 10 \* 4 Azure Integration Runtime（默认 DIU 设置 = 4）有关数据集成单元和副本性能优化的详细信息，请参阅[此文](copy-activity-performance.md) |
| 监视管道假设：仅发生 1 次运行 | 重试了 2 个监视运行记录（1 个用于管道运行，1 个用于活动运行） |

**方案定价总计：$0.16811**

- 数据工厂操作 = **$0.0001**
  - 读取/写入 = 10\*00001 = $0.0001 [1 读/写 = $0.50/50000 = 0.00001]
  - 监视  = 2\*000005 = $0.00001 [1 监视 = $0.25/50000 = 0.000005]
- 管道业务流程 &amp; 执行 = **$0.168**
  - 活动运行 = 001\*2 = 0.002 [1 运行 = $1/1000 = 0.001]
  - 数据移动活动 = $0.166（以 10 分钟的执行时间按比例计算。 Azure Integration Runtime 上的定价为 $0.25/小时）

## <a name="copy-data-and-transform-with-azure-databricks-hourly"></a>使用 Azure Databricks 按小时复制数据并进行转换

在此方案中，需使用 Azure Databricks 按计划将数据每隔一小时从 AWS S3 复制到 Azure Blob 存储并对数据进行转换。

若要完成此方案，需使用以下项创建一个管道：

1. 一个使用输入数据集（适用于将要从 AWS S3 复制的数据）和输出数据集（适用于 Azure 存储上的数据）的复制活动。
2. 一个用于数据转换的 Azure Databricks 活动。
3. 一个计划触发器，用于每隔一小时执行一次管道。

![方案 2](media/pricing-concepts/scenario2.png)

| **操作** | **类型和单元** |
| --- | --- |
| 创建链接的服务 | 3 个读/写实体  |
| 创建数据集 | 4 个读/写实体（2 个用于创建数据集，2 个用于链接的服务的引用） |
| 创建管道 | 3 个读/写实体（1 个用于创建管道，2 个用于数据集引用） |
| 获取管道 | 1 个读/写实体 |
| 运行管道 | 3 个活动运行（1 个用于触发器运行，2 个用于活动运行） |
| 复制数据假设：执行时间 = 10 分钟 | 10 \* 4 Azure Integration Runtime（默认 DIU 设置 = 4）有关数据集成单元和副本性能优化的详细信息，请参阅[此文](copy-activity-performance.md) |
| 监视管道假设：仅发生 1 次运行 | 重试了 3 个监视运行记录（1 个用于管道运行，2 个用于活动运行） |
| 执行 Databricks 活动假设：执行时间 = 10 分钟 | 10 分钟执行外部管道活动 |

**方案定价总计：$0.16916**

- 数据工厂操作 = **$0.00012**
  - 读取/写入 = 11\*00001 = $0.00011 [1 读/写 = $0.50/50000 = 0.00001]
  - 监视  = 3\*000005 = $0.00001 [1 监视 = $0.25/50000 = 0.000005]
- 管道业务流程 &amp; 执行 = **$0.16904**
  - 活动运行 = 001\*3 = 0.003 [1 运行 = $1/1000 = 0.001]
  - 数据移动活动 = $0.166（以 10 分钟的执行时间按比例计算。 Azure Integration Runtime 上的定价为 $0.25/小时）
  - 外部管道活动 = $0.000041（以 10 分钟的执行时间按比例计算。 Azure Integration Runtime 上的定价为 $0.00025/小时）

## <a name="copy-data-and-transform-with-dynamic-parameters-hourly"></a>使用动态参数按小时复制数据并进行转换

在此方案中，需使用 Azure Databricks（使用脚本中的动态参数）按计划将数据每隔一小时从 AWS S3 复制到 Azure Blob 存储并进行转换。

若要完成此方案，需使用以下项创建一个管道：

1. 一个使用输入数据集（适用于将要从 AWS S3 复制的数据）和输出数据集（适用于 Azure 存储上的数据）的复制活动。
2. 一个查找活动，用于将参数动态传递到转换脚本。
3. 一个用于数据转换的 Azure Databricks 活动。
4. 一个计划触发器，用于每隔一小时执行一次管道。

![方案 3](media/pricing-concepts/scenario3.png)

| **操作** | **类型和单元** |
| --- | --- |
| 创建链接的服务 | 3 个读/写实体  |
| 创建数据集 | 4 个读/写实体（2 个用于创建数据集，2 个用于链接的服务的引用） |
| 创建管道 | 3 个读/写实体（1 个用于创建管道，2 个用于数据集引用） |
| 获取管道 | 1 个读/写实体 |
| 运行管道 | 4 个活动运行（1 个用于触发器运行，3 个用于活动运行） |
| 复制数据假设：执行时间 = 10 分钟 | 10 \* 4 Azure Integration Runtime（默认 DIU 设置 = 4）有关数据集成单元和副本性能优化的详细信息，请参阅[此文](copy-activity-performance.md) |
| 监视管道假设：仅发生 1 次运行 | 重试了 4 个监视运行记录（1 个用于管道运行，3 个用于活动运行） |
| 执行查找活动假设：执行时间 = 1 分钟 | 1 分钟执行管道活动 |
| 执行 Databricks 活动假设：执行时间 = 10 分钟 | 10 分钟执行外部管道活动 |

**方案定价总计：$0.17020**

- 数据工厂操作 = **$0.00013**
  - 读取/写入 = 11\*00001 = $0.00011 [1 读/写 = $0.50/50000 = 0.00001]
  - 监视  = 4\*000005 = $0.00002 [1 监视 = $0.25/50000 = 0.000005]
- 管道业务流程 &amp; 执行 = **$0.17007**
  - 活动运行 = 001\*4 = 0.004 [1 运行 = $1/1000 = 0.001]
  - 数据移动活动 = $0.166（以 10 分钟的执行时间按比例计算。 Azure Integration Runtime 上的定价为 $0.25/小时）
  - 管道活动 = $0.00003（以 1 分钟的执行时间按比例计算。 Azure Integration Runtime 上的定价为 $0.002/小时）
  - 外部管道活动 = $0.000041（以 10 分钟的执行时间按比例计算。 Azure Integration Runtime 上的定价为 $0.00025/小时）

## <a name="using-mapping-data-flow-debug-for-a-normal-workday"></a>在正常工作日使用映射数据流调试

作为数据工程师，您每天负责设计、构建和测试映射数据流。 您可以在早上登录到 ADF UI，并为数据流启用调试模式。 调试会话的默认 TTL 为 60 分钟。 您全天工作 8 小时，因此调试会话永远不会过期。 因此，您当天的费用为：

**8 （小时） x 8 （计算优化的内核） x $0.193 = $12.35**

## <a name="transform-data-in-blob-store-with-mapping-data-flows"></a>使用映射数据流转换 Blob 存储中的数据

在这种情况下，您希望在 ADF 映射数据流中直观地转换 Blob 应用商店中的数据，

若要完成此方案，需使用以下项创建一个管道：

1. 具有转换逻辑的数据流活动。

2. Azure 存储上数据的输入数据集。

3. Azure 存储上的数据的输出数据集。

4. 一个计划触发器，用于每隔一小时执行一次管道。

| **操作** | **类型和单元** |
| --- | --- |
| 创建链接的服务 | 2 个读/写实体  |
| 创建数据集 | 4 个读/写实体（2 个用于创建数据集，2 个用于链接的服务的引用） |
| 创建管道 | 3 个读/写实体（1 个用于创建管道，2 个用于数据集引用） |
| 获取管道 | 1 个读/写实体 |
| 运行管道 | 2 个活动运行（1 个用于触发器运行，1 个用于活动运行） |
| 数据流假设：执行时间 = 10 分钟 = 10 分钟 TTL | 10 \* 16 个通用计算内核，TTL 为 10 |
| 监视管道假设：仅发生 1 次运行 | 重试了 2 个监视运行记录（1 个用于管道运行，1 个用于活动运行） |

**方案总价： $1.4631**

- 数据工厂操作 = **$0.0001**
  - 读取/写入 = 10\*00001 = $0.0001 [1 读/写 = $0.50/50000 = 0.00001]
  - 监视  = 2\*000005 = $0.00001 [1 监视 = $0.25/50000 = 0.000005]
- 管道编排&amp;执行 = **$1.463**
  - 活动运行 = 001\*2 = 0.002 [1 运行 = $1/1000 = 0.001]
  - 数据流活动 = 1.461 美元按比例 20 分钟（10 分钟执行时间 = 10 分钟 TTL）。 Azure 集成运行时的 0.274 美元/小时，具有 16 个核心常规计算

## <a name="next-steps"></a>后续步骤

了解 Azure 数据工厂的定价以后，即可开始操作！

- [使用 Azure 数据工厂 UI 创建数据工厂](quickstart-create-data-factory-portal.md)

- [Azure 数据工厂简介](introduction.md)

- [Azure 数据工厂中的可视化创作](author-visually.md)
