---
title: 通过示例了解 Azure 数据工厂定价
description: 本文使用详细的示例介绍并演示 Azure 数据工厂定价模型
documentationcenter: ''
author: djpmsft
ms.author: daperlov
manager: jroth
ms.reviewer: maghan
ms.service: data-factory
ms.workload: data-services
ms.topic: conceptual
ms.date: 12/27/2019
ms.openlocfilehash: d679dbb7a14767b83d6508e4b1e637584f33210a
ms.sourcegitcommit: e69bb334ea7e81d49530ebd6c2d3a3a8fa9775c9
ms.translationtype: MT
ms.contentlocale: zh-CN
ms.lasthandoff: 08/27/2020
ms.locfileid: "88949944"
---
# <a name="understanding-data-factory-pricing-through-examples"></a>通过示例了解数据工厂定价

[!INCLUDE[appliesto-adf-xxx-md](includes/appliesto-adf-xxx-md.md)]

本文使用详细的示例介绍并演示 Azure 数据工厂定价模型。

> [!NOTE]
> 以下这些示例中使用的价格是假设的，并不意味着实际定价。

## <a name="copy-data-from-aws-s3-to-azure-blob-storage-hourly"></a>将数据每隔一小时从 AWS S3 复制到 Azure Blob 存储

在此方案中，需按计划将数据每隔一小时从 AWS S3 复制到 Azure Blob 存储。

若要完成此方案，需使用以下项创建一个管道：

1. 使用输入数据集（适用于将要从 AWS S3 复制的数据）的复制活动。

2. Azure 存储上的数据的输出数据集。

3. 一个计划触发器，用于每隔一小时执行一次管道。

   ![方案 1](media/pricing-concepts/scenario1.png)

| **操作** | **类型和单元** |
| --- | --- |
| 创建链接的服务 | 2 个读/写实体  |
| 创建数据集 | 4 个读/写实体（2 个用于创建数据集，2 个用于链接的服务的引用） |
| 创建管道 | 3 个读/写实体（1 个用于创建管道，2 个用于数据集引用） |
| 获取管道 | 1 个读/写实体 |
| 运行管道 | 2 个活动运行（1 个用于触发器运行，1 个用于活动运行） |
| 复制数据假设：执行时间 = 10 分钟 | 10 \* 4 Azure Integration Runtime（默认 DIU 设置 = 4）有关数据集成单元和副本性能优化的详细信息，请参阅[此文](copy-activity-performance.md) |
| 监视管道假设：仅发生 1 次运行 | 重试了 2 个监视运行记录（1 个用于管道运行，1 个用于活动运行） |

**方案定价总计：$0.16811**

- 数据工厂操作 = **$0.0001**
  - 读取/写入 = 10\*00001 = $0.0001 [1 读/写 = $0.50/50000 = 0.00001]
  - 监视  = 2\*000005 = $0.00001 [1 监视 = $0.25/50000 = 0.000005]
- 管道业务流程 &amp; 执行 = **$0.168**
  - 活动运行 = 001\*2 = 0.002 [1 运行 = $1/1000 = 0.001]
  - 数据移动活动 = $0.166（以 10 分钟的执行时间按比例计算。 Azure Integration Runtime 上的定价为 $0.25/小时）

## <a name="copy-data-and-transform-with-azure-databricks-hourly"></a>使用 Azure Databricks 按小时复制数据并进行转换

在此方案中，需使用 Azure Databricks 按计划将数据每隔一小时从 AWS S3 复制到 Azure Blob 存储并对数据进行转换。

若要完成此方案，需使用以下项创建一个管道：

1. 一个使用输入数据集（适用于将要从 AWS S3 复制的数据）和输出数据集（适用于 Azure 存储上的数据）的复制活动。
2. 一个用于数据转换的 Azure Databricks 活动。
3. 一个计划触发器，用于每隔一小时执行一次管道。

![方案 2](media/pricing-concepts/scenario2.png)

| **操作** | **类型和单元** |
| --- | --- |
| 创建链接的服务 | 3 个读/写实体  |
| 创建数据集 | 4 个读/写实体（2 个用于创建数据集，2 个用于链接的服务的引用） |
| 创建管道 | 3 个读/写实体（1 个用于创建管道，2 个用于数据集引用） |
| 获取管道 | 1 个读/写实体 |
| 运行管道 | 3 个活动运行（1 个用于触发器运行，2 个用于活动运行） |
| 复制数据假设：执行时间 = 10 分钟 | 10 \* 4 Azure Integration Runtime（默认 DIU 设置 = 4）有关数据集成单元和副本性能优化的详细信息，请参阅[此文](copy-activity-performance.md) |
| 监视管道假设：仅发生 1 次运行 | 重试了 3 个监视运行记录（1 个用于管道运行，2 个用于活动运行） |
| 执行 Databricks 活动假设：执行时间 = 10 分钟 | 10 分钟执行外部管道活动 |

**方案定价总计：$0.16916**

- 数据工厂操作 = **$0.00012**
  - 读取/写入 = 11\*00001 = $0.00011 [1 读/写 = $0.50/50000 = 0.00001]
  - 监视  = 3\*000005 = $0.00001 [1 监视 = $0.25/50000 = 0.000005]
- 管道业务流程 &amp; 执行 = **$0.16904**
  - 活动运行 = 001\*3 = 0.003 [1 运行 = $1/1000 = 0.001]
  - 数据移动活动 = $0.166（以 10 分钟的执行时间按比例计算。 Azure Integration Runtime 上的定价为 $0.25/小时）
  - 外部管道活动 = $0.000041（以 10 分钟的执行时间按比例计算。 Azure Integration Runtime 上的定价为 $0.00025/小时）

## <a name="copy-data-and-transform-with-dynamic-parameters-hourly"></a>使用动态参数按小时复制数据并进行转换

在此方案中，需使用 Azure Databricks（使用脚本中的动态参数）按计划将数据每隔一小时从 AWS S3 复制到 Azure Blob 存储并进行转换。

若要完成此方案，需使用以下项创建一个管道：

1. 一个使用输入数据集（适用于将要从 AWS S3 复制的数据）和输出数据集（适用于 Azure 存储上的数据）的复制活动。
2. 一个查找活动，用于将参数动态传递到转换脚本。
3. 一个用于数据转换的 Azure Databricks 活动。
4. 一个计划触发器，用于每隔一小时执行一次管道。

![方案 3](media/pricing-concepts/scenario3.png)

| **操作** | **类型和单元** |
| --- | --- |
| 创建链接的服务 | 3 个读/写实体  |
| 创建数据集 | 4 个读/写实体（2 个用于创建数据集，2 个用于链接的服务的引用） |
| 创建管道 | 3 个读/写实体（1 个用于创建管道，2 个用于数据集引用） |
| 获取管道 | 1 个读/写实体 |
| 运行管道 | 4 个活动运行（1 个用于触发器运行，3 个用于活动运行） |
| 复制数据假设：执行时间 = 10 分钟 | 10 \* 4 Azure Integration Runtime（默认 DIU 设置 = 4）有关数据集成单元和副本性能优化的详细信息，请参阅[此文](copy-activity-performance.md) |
| 监视管道假设：仅发生 1 次运行 | 重试了 4 个监视运行记录（1 个用于管道运行，3 个用于活动运行） |
| 执行查找活动假设：执行时间 = 1 分钟 | 1 分钟执行管道活动 |
| 执行 Databricks 活动假设：执行时间 = 10 分钟 | 10 分钟执行外部管道活动 |

**方案定价总计：$0.17020**

- 数据工厂操作 = **$0.00013**
  - 读取/写入 = 11\*00001 = $0.00011 [1 读/写 = $0.50/50000 = 0.00001]
  - 监视  = 4\*000005 = $0.00002 [1 监视 = $0.25/50000 = 0.000005]
- 管道业务流程 &amp; 执行 = **$0.17007**
  - 活动运行 = 001\*4 = 0.004 [1 运行 = $1/1000 = 0.001]
  - 数据移动活动 = $0.166（以 10 分钟的执行时间按比例计算。 Azure Integration Runtime 上的定价为 $0.25/小时）
  - 管道活动 = $0.00003（以 1 分钟的执行时间按比例计算。 Azure Integration Runtime 上的定价为 $0.002/小时）
  - 外部管道活动 = $0.000041（以 10 分钟的执行时间按比例计算。 Azure Integration Runtime 上的定价为 $0.00025/小时）

## <a name="using-mapping-data-flow-debug-for-a-normal-workday"></a>对普通 workday 使用映射数据流调试

作为一位数据工程人员，你负责设计、构建和测试每天映射数据流。 你将在早上登录 ADF UI，并为数据流启用调试模式。 调试会话的默认 TTL 为60分钟。 你一整天就会工作8小时，因此调试会话永远不会过期。 因此，一天的费用将为：

**8 (小时) x 8 (计算优化内核) x $0.193 = $12.35**

## <a name="transform-data-in-blob-store-with-mapping-data-flows"></a>在 blob 存储区中转换数据和映射数据流

在这种情况下，你想要在 ADF 中直观地转换 Blob 存储中的数据，按小时计划。

若要完成此方案，需使用以下项创建一个管道：

1. 带有转换逻辑的数据流活动。

2. Azure 存储上数据的输入数据集。

3. Azure 存储上的数据的输出数据集。

4. 一个计划触发器，用于每隔一小时执行一次管道。

| **操作** | **类型和单元** |
| --- | --- |
| 创建链接的服务 | 2 个读/写实体  |
| 创建数据集 | 4 个读/写实体（2 个用于创建数据集，2 个用于链接的服务的引用） |
| 创建管道 | 3 个读/写实体（1 个用于创建管道，2 个用于数据集引用） |
| 获取管道 | 1 个读/写实体 |
| 运行管道 | 2 个活动运行（1 个用于触发器运行，1 个用于活动运行） |
| 数据流假设：执行时间 = 10 分钟 + 10 分钟 TTL | \*带有 TTL 10 的一般计算的 10 16 核心 |
| 监视管道假设：仅发生 1 次运行 | 重试了 2 个监视运行记录（1 个用于管道运行，1 个用于活动运行） |

**方案总定价： $1.4631**

- 数据工厂操作 = **$0.0001**
  - 读取/写入 = 10\*00001 = $0.0001 [1 读/写 = $0.50/50000 = 0.00001]
  - 监视  = 2\*000005 = $0.00001 [1 监视 = $0.25/50000 = 0.000005]
- 管道业务流程 &amp; 执行 = **$1.463**
  - 活动运行 = 001\*2 = 0.002 [1 运行 = $1/1000 = 0.001]
  - 数据流活动 = $1.461 按比例20分钟 (10 分钟执行时间 + 10 分钟 TTL) 。 Azure Integration Runtime 上的 $ 0.274/小时，具有16个核心一般计算

## <a name="data-integration-in-azure-data-factory-managed-vnet"></a>Azure 数据工厂托管 VNET 中的数据集成
在这种情况下，你想要删除 Azure Blob 存储中的原始文件并将数据从 Azure SQL 数据库复制到 Azure Blob 存储。 你将在不同的管道上执行两次此执行。 这两个管道的执行时间重叠。
![Scenario4 ](media/pricing-concepts/scenario-4.png) 若要实现此方案，需要创建具有以下项的两个管道：
  - 管道活动–删除活动。
  - 一个复制活动，其中包含要从 Azure Blob 存储复制的数据的输入数据集。
  - 用于 Azure SQL 数据库上的数据的输出数据集。
  - 用于执行管道的计划触发器。


| **操作** | **类型和单元** |
| --- | --- |
| 创建链接的服务 | 4读取/写入实体 |
| 创建数据集 | 8个读取/写入实体 (4 个，用于创建数据集，4个用于链接服务引用)  |
| 创建管道 | 6用于创建管道的 (2 读取/写入实体，4表示数据集引用)  |
| 获取管道 | 2 个读/写实体 |
| 运行管道 | 6活动运行 (2 用于触发器运行，4表示活动运行)  |
| 执行删除活动：每个执行时间 = 5 分钟第一条管道中的 Delete 活动执行时间为 10:00 AM UTC 到 10:05 AM UTC。 第二个管道中的 Delete 活动执行时间为 10:02 AM UTC 到 10:07 AM UTC。|托管 VNET 中7分钟内的最小活动执行。 在托管 VNET 中，管道活动最多支持50并发。 |
| 复制数据假设：每次执行时间 = 10 分钟。第一条管道中的复制执行时间为 10:06 AM UTC 到 10:15 AM UTC。 第二个管道中的 Delete 活动执行时间为 10:08 AM UTC 到 10:17 AM UTC。 | 10 * 4 Azure Integration Runtime (默认 DIU 设置 = 4) 有关数据集成单元和优化复制性能的详细信息，请参阅 [此文](copy-activity-performance.md) |
| 监视管道假设：只发生2个运行 | 6监视运行记录重试 (2 对于管道运行，4表示活动运行)  |


**方案总定价： $0.45523**

- 数据工厂操作 = $0.00023
  - 读/写 = 20 * 00001 = $0.0002 [1 R/W = $ 0.50/50000 = 0.00001]
  - 监视 = 6 * 000005 = $0.00003 [1 监视 = $ 0.25/50000 = 0.000005]
- 管道业务流程 & 执行 = $0.455
  - 活动运行 = 0.001 * 6 = 0.006 [1 运行 = $ 1/1000 = 0.001]
  - 数据移动活动 = $0.333 (按比例执行时间为10分钟。 Azure Integration Runtime 上的定价为 $0.25/小时）
  - 管道活动 = $0.116 (按7分钟执行时间。 Azure Integration Runtime 上的 $ 1/小时) 

> [!NOTE]
> 这些价格仅用于示例目的。

**常见问题解答**

问：如果我想要运行超过50的管道活动，可以同时执行这些活动吗？

答：允许最大50并发管道活动。  51th 管道活动将排入队列，直到打开 "自由槽"。 对于外部活动是相同的。 允许最大800个并发外部活动。

## <a name="next-steps"></a>后续步骤

了解 Azure 数据工厂的定价以后，即可开始操作！

- [使用 Azure 数据工厂 UI 创建数据工厂](quickstart-create-data-factory-portal.md)

- [Azure 数据工厂简介](introduction.md)

- [Azure 数据工厂中的视觉对象创作](author-visually.md)
