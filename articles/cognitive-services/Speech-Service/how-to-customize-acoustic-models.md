---
title: 教程：使用语音服务创建声学模型
titlesuffix: Azure Cognitive Services
description: 了解如何使用 Azure 认知服务中的语音服务创建声学模型。
services: cognitive-services
author: PanosPeriorellis
manager: nitinme
ms.service: cognitive-services
ms.subservice: speech-service
ms.topic: tutorial
ms.date: 06/25/2018
ms.author: panosper
ms.openlocfilehash: b644d1d227b5dbd69af38cc32defffb8152b0cde
ms.sourcegitcommit: 90cec6cccf303ad4767a343ce00befba020a10f6
ms.translationtype: HT
ms.contentlocale: zh-CN
ms.lasthandoff: 02/07/2019
ms.locfileid: "55878113"
---
# <a name="tutorial-create-a-custom-acoustic-model"></a>教程：创建自定义声学模型

如果应用程序设计用于特定的环境（例如汽车）、特定的录制设备或条件，或特定的用户群，则创建自定义声学模型会很有帮助。 示例包括带有口音的讲话、特定的背景噪音，或使用特定的麦克风录制音频。

在本文中，学习如何：
> [!div class="checklist"]
> * 准备数据
> * 导入声学数据集
> * 创建自定义声学模型

如果没有 Azure 认知服务帐户，请在开始前创建一个[免费帐户](https://azure.microsoft.com/try/cognitive-services)。

## <a name="prerequisites"></a>先决条件

打开[认知服务订阅](https://cris.ai/Subscriptions)页，确保认知服务帐户已连接到某个订阅。

可以选择“连接现有订阅”，连接到在 Azure 门户中创建的语音服务订阅。

有关在 Azure 门户中创建语音服务订阅的信息，请参阅[免费试用语音服务](get-started.md)。

## <a name="prepare-the-data"></a>准备数据

若要自定义特定域的声学模型，需要一系列语音数据。 此集合可以是几条话语，也可以是几百个小时的讲话。 此集合包括一组语音数据音频文件，以及每个音频文件的听录文本文件。 音频数据应能够代表要使用识别器的场景。

例如：

* 若要更好地识别嘈杂工厂环境中的语音，音频文件应包括嘈杂工厂中的讲话人。
* 如果想要优化单个讲话人的性能（例如，罗斯福总统的所有炉边谈话），可以听录他的所有言语，然后，音频文件应该只会包括该讲话人的多个示例。

用于自定义声学模型的声学数据集由两个部分组成：(1) 一组包含语音数据的音频文件；(2) 一个包含所有音频文件的听录内容的文件。

### <a name="audio-data-recommendations"></a>音频数据建议

* 应该以 WAV (RIFF) 音频格式存储数据集中的所有音频文件。
* 音频的采样率必须是 8 KHz 或 16 KHz，样本值应存储为未压缩的脉冲码调制 (PCM) 16 位带符号整数（短型）。
* 支持仅单声道（单音）音频文件。
* 音频文件的长度可以为 100 毫秒到 1 分钟，不过理想情况下应该在 10-12 秒左右。 每个音频文件的开头和末尾最好是至少有 100 毫秒的寂静区（往往是 500 毫秒到 1 秒）。
* 如果数据中包含背景噪音，则我们建议在数据中讲话内容的前面和/或后面另外包含一些具有更长寂静区的示例（例如，几秒钟的寂静区）。
* 每个音频文件应包含一条话语（例如，一条听写句子）、一个查询，或者对话系统的一个轮次。
* 数据集中的每个音频文件应有唯一的文件名和扩展名 .wav。
* 音频文件集应放置到不带子目录的单个文件夹中，整个音频文件集应打包为单个 .zip 文件存档。

> [!NOTE]
> 通过 Web 门户导入的数据目前限制为 2 GB，因此，这是声学数据集的最大大小。 此大小相当于以 16 KHz 频率录制了 17 个小时的音频，或者以 8 KHz 的频率录制了 34 个小时的音频。 下表汇总了音频数据的主要要求：
>

| 属性 | 值 |
|---------- |----------|
| 文件格式 | RIFF (WAV) |
| 采样率 | 8,000 Hz 或 16,000 Hz |
| 声道 | 1（单音） |
| 样本格式 | PCM，16 位整数 |
| 文件持续时间 | 0.1 秒 < 持续时间 < 12 秒 | 
| 无声凸环 | > 0.1 秒 |
| 存档格式 | .zip |
| 最大存档大小 | 2 GB |

> [!NOTE]
> 文件名只能使用拉丁字符，采用“文件名.扩展名”格式

## <a name="language-support"></a>语言支持

有关自定义“语音转文本”语言模型支持的完整语言列表，请参阅[语音服务支持的语言](language-support.md#speech-to-text)。

### <a name="transcriptions-for-the-audio-dataset"></a>音频数据集的听录

应在单个纯文本文件中包含所有 WAV 文件的听录。 听录文件的每一行应包含一个音频文件的名称，后接相应的听录。 文件名和听录应以制表符 (\t) 分隔。

  例如：
```
  speech01.wav  speech recognition is awesome
  speech02.wav  the quick brown fox jumped all over the place
  speech03.wav  the lazy dog was not amused
```
> [!NOTE]
> 听录应编码为 UTF-8 字节顺序标记 (BOM)。

听录内容应经过文本规范化，以便可由系统处理。 但是，用户在将数据上传到自定义语音服务之前，必须完成一些重要的规范化操作。 有关准备听录内容时可用的语言，请参阅[使用语音服务时的听录准则](prepare-transcription.md)。

使用[语音服务门户](https://cris.ai)来执行下一部分所述的步骤。

## <a name="import-the-acoustic-dataset"></a>导入声学数据集

准备好音频文件和听录内容后，可将其导入服务 Web 门户。

若要导入这些文件，请先确保登录到[语音服务门户](https://cris.ai)。 然后，在功能区上的“自定义语音”下拉菜单中，选择“适应数据”。 如果这是你第一次将数据上传到自定义语音服务，则会显示带有“数据集”标签的空表。 

在“声学数据集”行中单击“导入”按钮，此时站点会显示一个用于上传新数据集的页面。

![“导入声学数据”页](media/stt/speech-acoustic-datasets-import.png)

在“名称”和“说明”框中输入相应的信息。 友好的说明有助于跟踪上传的各种数据集。 

在“听录文件 (.txt)”和“音频文件(.zip)”框中选择“浏览”，然后选择纯文本听录文件和 WAV 文件的 zip 存档。 准备工作完成后，选择“导入”以上传数据。 随即会上传数据。 对于较大数据集，导入过程可能需要花费几分钟时间。

上传完成后，返回到“声学数据集”表。 此时会显示一个对应于声学数据集的条目。 请注意，为该条目分配了唯一 ID (GUID)。 数据会显示其当前状态：如果正在排队等待处理，则状态为“未启动”；如果正在进行验证，则状态为“正在运行”；如果数据已做好使用准备，则状态为“完成”。

数据验证包括针对音频文件执行一系列检查以验证文件格式、长度和采样率，并针对听录文件执行一系列检查以验证文件格式和执行一些文本规范化操作。

当状态为“成功”时，可以选择“详细信息”查看声学数据验证报告。 将会显示通过验证和未通过验证的陈述的数目，以及未通过验证的陈述的详细信息。 在下图的示例中，有两个 WAV 文件由于音频格式不正确而未通过验证。 在此数据集中，一个文件的采样率不正确，另一个文件的文件格式不正确。

![“适应数据详细信息”页](media/stt/speech-acoustic-datasets-report.png)

若要更改数据集的名称或说明，可以选择“编辑”链接并更改其对应的条目。 无法修改听录或音频文件条目。

## <a name="create-a-custom-acoustic-model"></a>创建自定义声学模型

当声学数据集的状态为“完成”后，可以使用该数据集来创建自定义声学模型。 为此，请在“自定义语音”下拉列表中选择“声学模型”。 标有“你的模型”的表会列出所有自定义声学模型。 首次使用时，此表为空。 表标题显示当前区域设置。 目前，只能为“美国英语”创建声学模型。

若要创建新模型，请选择表标题下的“新建”。 像前面一样，输入名称和说明以帮助识别此模型。 例如，可以使用“说明”字段来记录创建模型时使用了哪个起始模型和声学数据集。 

接下来，在“基本声学模型”下拉列表中选择一个基本模型。 该基本模型是自定义操作的起点。 有两个基本声学模型可供选择：
* “Microsoft 搜索和听写 AM”模型适用于在应用程序中发出的语音，例如命令、搜索查询或听写。 
* “Microsoft 聊天模型”适用于识别以对话形式说出的语音。 此类语音通常是面对另一个人在呼叫中心或会议中发出。 

“对话”模型中部分结果的延迟高于“搜索”和“听写”模型中的延迟。

> [!NOTE]
> 我们即将推出新的“通用”模型，旨在解决所有方案的要求。 到时，上述模型仍可公开使用。

接下来，在“声学数据”下拉列表中，选择要用来执行自定义操作的声学数据。

![“创建声学模型”页](media/stt/speech-acoustic-models-create2.png)

完成处理后，可根据需要选择对新模型执行准确度测试。 这会使用自定义的声学模型针对指定的声学数据集测试运行语音转文本评估，然后报告结果。 若要执行此测试，请选中“准确度测试”复选框。 然后，在下拉菜单中选择一个语言模型。 如果尚未创建任何自定义语言模型，则下拉列表中只会显示基本语言模型。 若要选择最合适的语言模型，请参阅[教程：创建自定义语言模型](how-to-customize-language-model.md)。

最后，选择要用来评估自定义模型的声学数据集。 如果执行准确度测试，选择的声学数据集必须不同于创建模型时所用的数据集，这样才能获得真正有意义的模型性能。 基于训练数据测试准确度并不能评估修改后的模型在实际条件下的性能。 结果会过于乐观。 另请注意，准确度测试限制为 1,000 条话语。 如果要测试的声学数据集较大，则只会评估最前面的 1,000 条话语。

准备好开始运行自定义过程时，请选择“创建”。

声学模型表将显示对应于此新模型的新条目。 该表还会显示过程的状态：“正在等待”、“正在处理”或“完成”。

![“声学模型”页](media/stt/speech-acoustic-models-creating.png)

## <a name="next-steps"></a>后续步骤

- [获取语音服务试用订阅](https://azure.microsoft.com/try/cognitive-services/)
- [在 C# 中识别语音](quickstart-csharp-dotnet-windows.md)
- [Git 示例数据](https://github.com/Microsoft/Cognitive-Custom-Speech-Service)
